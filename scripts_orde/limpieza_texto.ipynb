{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d448050",
   "metadata": {},
   "source": [
    "# Pipeline de preprocesamiento de texto (con Pandas + NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85cab2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pzambonino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ===============================================\n",
    "# ‚öôÔ∏è DESCARGAR RECURSOS NECESARIOS (solo 1 vez)\n",
    "# ===============================================\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"omw-1.4\", quiet=True)  # mejora la lematizaci√≥n\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bc013c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset cargado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# üìÇ CARGAR DATASET LIMPIO\n",
    "# ===============================================\n",
    "try:\n",
    "    df = pd.read_csv(\"amazon_reviews_clean.csv\")\n",
    "    print(\"‚úÖ Dataset cargado correctamente.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"‚ùå No se encontr√≥ el archivo 'amazon_reviews_clean.csv'. \"\n",
    "                            \"Verifica la ruta o nombre del archivo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73d493a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üßπ 1. UNIFICAR TEXTO (Resumen + Rese√±a)\n",
    "# ===============================================\n",
    "df[\"FullReview\"] = df[\"Summary\"].fillna(\"\").astype(str) + \" \" + df[\"Text\"].fillna(\"\").astype(str)\n",
    "\n",
    "# ===============================================\n",
    "# üßΩ 2. LIMPIEZA DE TEXTO\n",
    "# ===============================================\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Limpia el texto: min√∫sculas, sin URLs, sin signos, sin n√∫meros.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)      # URLs\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)                    # solo letras y espacios\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()                 # espacios extra\n",
    "    return text\n",
    "\n",
    "df[\"CleanText\"] = df[\"FullReview\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19a29cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pzambonino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pzambonino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pzambonino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# üîΩ Descargar los recursos necesarios de NLTK (solo la primera vez)\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")  # <- importante\n",
    "\n",
    "# ===============================================\n",
    "# üö´ 3. ELIMINAR STOPWORDS (palabras vac√≠as)\n",
    "# ===============================================\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([w for w in tokens if w not in stop_words])\n",
    "\n",
    "# Aplica la funci√≥n a tu columna\n",
    "df[\"CleanText\"] = df[\"CleanText\"].apply(remove_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c7e1d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Preprocesamiento completado. Archivo guardado como 'amazon_reviews_prepared.csv'.\n",
      "üìä Vista previa de los primeros registros:\n",
      "    ProductId  Score        Time  \\\n",
      "0  B001E4KFG0      5  2011-04-27   \n",
      "1  B000LQOCH0      4  2008-08-18   \n",
      "2  B000UA0QIQ      2  2011-06-13   \n",
      "3  B000E7L2R4      5  2011-11-23   \n",
      "4  B0001PB9FE      5  2005-02-08   \n",
      "\n",
      "                                           CleanText  HelpfulLabel  \n",
      "0  good quality dog food bought several vitality ...             1  \n",
      "1  delight say confection around century light pi...             1  \n",
      "2  cough medicine looking secret ingredient robit...             1  \n",
      "3  yay barley right mostly sprouting cat eat gras...             1  \n",
      "4  best hot sauce world know cactus tequila uniqu...             1  \n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text: str) -> str:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "df[\"CleanText\"] = df[\"CleanText\"].apply(lemmatize_text)\n",
    "\n",
    "# ===============================================\n",
    "# üíæ 5. GUARDAR DATASET PROCESADO\n",
    "# ===============================================\n",
    "cols_to_keep = [\"ProductId\", \"Score\", \"Time\", \"CleanText\", \"HelpfulLabel\"]\n",
    "missing_cols = [col for col in cols_to_keep if col not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"‚ö†Ô∏è Advertencia: Faltan columnas en el dataset: {missing_cols}\")\n",
    "    df_prepared = df[[c for c in cols_to_keep if c in df.columns]]\n",
    "else:\n",
    "    df_prepared = df[cols_to_keep]\n",
    "\n",
    "output_path = \"amazon_reviews_prepared.csv\"\n",
    "df_prepared.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocesamiento completado. Archivo guardado como '{output_path}'.\")\n",
    "print(\"üìä Vista previa de los primeros registros:\")\n",
    "print(df_prepared.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
